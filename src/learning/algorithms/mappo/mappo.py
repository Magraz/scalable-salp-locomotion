import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np

from learning.algorithms.mappo.types import Params
from learning.algorithms.mappo.network import MAPPONetwork


class MAPPOAgent:
    """Multi-Agent PPO with centralized critic"""

    def __init__(
        self,
        observation_dim: int,
        global_state_dim: int,
        action_dim: int,
        n_agents: int,
        params: Params,
        device: str,
        discrete: bool,
        n_parallel_envs: int,
    ):
        self.device = device
        self.n_agents = n_agents
        self.observation_dim = observation_dim
        self.global_state_dim = global_state_dim
        self.discrete = discrete
        self.share_actor = params.parameter_sharing
        self.n_parallel_envs = n_parallel_envs

        # PPO hyperparameters
        self.gamma = params.gamma
        self.gae_lambda = params.lmbda
        self.clip_epsilon = params.eps_clip
        self.entropy_coef = params.ent_coef
        self.value_coef = params.val_coef
        self.grad_clip = params.grad_clip

        # Create network
        self.network = MAPPONetwork(
            observation_dim=observation_dim,
            global_state_dim=global_state_dim,
            action_dim=action_dim,
            n_agents=n_agents,
            discrete=discrete,
            share_actor=self.share_actor,
        ).to(device)

        # Create old network for PPO
        self.network_old = MAPPONetwork(
            observation_dim=observation_dim,
            global_state_dim=global_state_dim,
            action_dim=action_dim,
            n_agents=n_agents,
            discrete=discrete,
            share_actor=self.share_actor,
        ).to(device)

        self.network_old.load_state_dict(self.network.state_dict())

        # Optimizer for all parameters
        self.optimizer = optim.Adam(self.network.parameters(), lr=params.lr)

        # Buffers for each agent
        self.reset_buffers()

    def reset_buffers(self):
        """Reset experience buffers - separate for each parallel environment"""
        # Buffers indexed by [env_idx][agent_idx]
        self.observations = [
            [[] for _ in range(self.n_agents)] for _ in range(self.n_parallel_envs)
        ]
        self.global_states = [[] for _ in range(self.n_parallel_envs)]
        self.actions = [
            [[] for _ in range(self.n_agents)] for _ in range(self.n_parallel_envs)
        ]
        self.rewards = [
            [[] for _ in range(self.n_agents)] for _ in range(self.n_parallel_envs)
        ]
        self.log_probs = [
            [[] for _ in range(self.n_agents)] for _ in range(self.n_parallel_envs)
        ]
        self.values = [[] for _ in range(self.n_parallel_envs)]
        self.dones = [
            [[] for _ in range(self.n_agents)] for _ in range(self.n_parallel_envs)
        ]

    def get_actions(self, observations, global_state, deterministic=False):
        """
        Get actions for all agents

        Args:
            observations: List of observations, one per agent
            global_state: Concatenated global state (all agent observations)
            deterministic: Whether to use deterministic actions

        Returns:
            actions: List of actions for each agent
            log_probs: List of log probabilities
            value: Single value from centralized critic
        """
        with torch.no_grad():
            # Convert observations to tensors
            obs_tensors = [
                torch.FloatTensor(obs).to(self.device) for obs in observations
            ]

            # Convert global state to tensor
            global_state_tensor = torch.FloatTensor(global_state).to(self.device)

            # Get actions from each actor
            actions = []
            log_probs = []

            for agent_idx, obs_tensor in enumerate(obs_tensors):
                action, log_prob = self.network_old.act(
                    obs_tensor, agent_idx, deterministic
                )
                actions.append(action)
                log_probs.append(log_prob)

            # Get value from centralized critic
            value = self.network_old.get_value(global_state_tensor)

        return torch.stack(actions), torch.cat(log_probs), value

    def store_transition(
        self,
        env_idx,
        observations,
        global_state,
        actions,
        rewards,
        log_probs,
        value,
        dones,
    ):
        """Store transition for a specific environment"""
        # Store global state (shared)
        self.global_states[env_idx].append(
            torch.FloatTensor(global_state).to(self.device)
        )

        # Store value (shared)
        self.values[env_idx].append(
            torch.tensor(value, dtype=torch.float32).to(self.device)
        )

        # Store per-agent data
        for agent_idx in range(self.n_agents):
            # Observation
            self.observations[env_idx][agent_idx].append(
                torch.FloatTensor(observations[agent_idx]).to(self.device)
            )

            # Action
            self.actions[env_idx][agent_idx].append(
                torch.FloatTensor(actions[agent_idx]).to(self.device)
            )

            # Reward, log_prob, done
            self.rewards[env_idx][agent_idx].append(
                torch.tensor(rewards[agent_idx], dtype=torch.float32).to(self.device)
            )
            self.log_probs[env_idx][agent_idx].append(
                torch.tensor(log_probs[agent_idx], dtype=torch.float32).to(self.device)
            )
            self.dones[env_idx][agent_idx].append(
                torch.tensor(dones[agent_idx], dtype=torch.float32).to(self.device)
            )

    def compute_returns_and_advantages(self, next_values):
        """
        Compute returns and advantages using per-environment final values

        Args:
            next_values: List or array of final values, one per environment
        """
        all_returns = []
        all_advantages = []

        # Process each environment separately
        for env_idx in range(self.n_parallel_envs):
            if len(self.values[env_idx]) == 0:
                continue  # Skip if no data for this env

            next_value = next_values[env_idx]
            if not torch.is_tensor(next_value):
                next_value = torch.tensor(next_value, dtype=torch.float32).to(
                    self.device
                )

            # Use values from this environment's trajectory
            env_values = self.values[env_idx][:]
            env_values.append(next_value.unsqueeze(0))
            env_values = torch.cat(env_values)

            # Compute advantages for each agent in this environment
            for agent_idx in range(self.n_agents):
                if len(self.rewards[env_idx][agent_idx]) == 0:
                    continue

                rewards = torch.stack(self.rewards[env_idx][agent_idx])
                dones = torch.cat(
                    [
                        torch.stack(self.dones[env_idx][agent_idx]),
                        torch.zeros(1, device=self.device),
                    ]
                )

                # Initialize advantages
                advantages = torch.zeros_like(rewards)

                # Compute GAE
                gae = torch.tensor(0.0, device=self.device)
                for step in reversed(range(len(rewards))):
                    delta = (
                        rewards[step]
                        + self.gamma * env_values[step + 1] * (1 - dones[step])
                        - env_values[step]
                    )
                    gae = delta + self.gamma * self.gae_lambda * (1 - dones[step]) * gae
                    advantages[step] = gae

                # Compute returns
                returns = advantages + env_values[:-1]

                all_returns.append(returns.detach())
                all_advantages.append(advantages.detach())

        return all_returns, all_advantages

    def update_shared(
        self,
        all_advantages,
        all_returns,
        minibatch_size,
        epochs,
    ):

        # Training statistics
        stats = {"total_loss": 0, "policy_loss": 0, "value_loss": 0, "entropy_loss": 0}
        num_updates = 0

        # Combine all agent data for shared actor update
        all_obs = []
        all_global_states = []
        all_actions = []
        all_old_log_probs = []
        all_returns_combined = []
        all_advantages_combined = []

        # Iterate through each environment
        for env_idx in range(self.n_parallel_envs):
            if len(self.values[env_idx]) == 0:
                continue  # Skip empty environments

            # For this environment, iterate through each agent
            for agent_idx in range(self.n_agents):
                if len(self.observations[env_idx][agent_idx]) == 0:
                    continue  # Skip if no data for this agent

                # Get the index in the flattened advantages/returns list
                # The compute_returns_and_advantages returns a flat list combining all envs and agents
                data_idx = env_idx * self.n_agents + agent_idx

                if data_idx >= len(all_advantages):
                    continue

                # Normalize advantages for this agent in this environment
                advantages = all_advantages[data_idx]
                advantages = (advantages - advantages.mean()) / (
                    advantages.std() + 1e-8
                )

                # Stack data for this agent in this environment
                obs = torch.stack(self.observations[env_idx][agent_idx])
                actions = torch.stack(self.actions[env_idx][agent_idx])
                old_log_probs = torch.stack(self.log_probs[env_idx][agent_idx])
                returns = all_returns[data_idx]

                # Get corresponding global states (repeated for each agent's timestep)
                global_states = torch.stack(self.global_states[env_idx])

                # Append to combined lists
                all_obs.append(obs)
                all_global_states.append(global_states)
                all_actions.append(actions)
                all_old_log_probs.append(old_log_probs)
                all_returns_combined.append(returns)
                all_advantages_combined.append(advantages)

        # Concatenate all data
        combined_obs = torch.cat(all_obs, dim=0).detach()
        combined_global_states = torch.cat(all_global_states, dim=0).detach()
        combined_actions = torch.cat(all_actions, dim=0).detach()
        combined_old_log_probs = torch.cat(all_old_log_probs, dim=0).detach()
        combined_returns = torch.cat(all_returns_combined, dim=0)
        combined_advantages = torch.cat(all_advantages_combined, dim=0)

        # Create dataset
        dataset = TensorDataset(
            combined_obs,
            combined_global_states,
            combined_actions,
            combined_old_log_probs,
            combined_returns,
            combined_advantages,
        )

        dataloader = DataLoader(dataset, batch_size=minibatch_size, shuffle=True)

        # Train for multiple epochs
        for epoch in range(epochs):
            for batch in dataloader:
                (
                    batch_obs,
                    batch_global_states,
                    batch_actions,
                    batch_old_log_probs,
                    batch_returns,
                    batch_advantages,
                ) = batch

                # We need to know which agent each sample belongs to
                # For shared actor, we can use agent_idx = 0 (same for all)
                log_probs, values, entropy = self.network.evaluate_actions(
                    batch_obs, batch_global_states, batch_actions, agent_idx=0
                )

                # PPO objective
                ratio = torch.exp(log_probs.squeeze(-1) - batch_old_log_probs)
                surr1 = ratio * batch_advantages
                surr2 = (
                    torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon)
                    * batch_advantages
                )
                policy_loss = -torch.min(surr1, surr2).mean()

                # Value loss
                value_loss = F.mse_loss(values, batch_returns)

                # Entropy loss
                entropy_loss = -entropy.mean()

                # Total loss
                loss = (
                    policy_loss
                    + self.value_coef * value_loss
                    + self.entropy_coef * entropy_loss
                )

                # Optimize
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    self.network.parameters(), self.grad_clip
                )
                self.optimizer.step()

                # Update statistics
                stats["total_loss"] += loss.item()
                stats["policy_loss"] += policy_loss.item()
                stats["value_loss"] += value_loss.item()
                stats["entropy_loss"] += entropy_loss.item()
                num_updates += 1

        return stats, num_updates

    def update_independent_actors(
        self,
        all_advantages,
        all_returns,
        minibatch_size,
        epochs,
    ):

        # Training statistics
        stats = {"total_loss": 0, "policy_loss": 0, "value_loss": 0, "entropy_loss": 0}
        num_updates = 0

        # Update each agent separately (independent actors)
        for agent_idx in range(self.n_agents):
            # Collect data for this agent across all environments
            agent_obs = []
            agent_global_states = []
            agent_actions = []
            agent_old_log_probs = []
            agent_returns = []
            agent_advantages = []

            for env_idx in range(self.n_parallel_envs):
                if len(self.observations[env_idx][agent_idx]) == 0:
                    continue  # Skip if no data for this agent in this environment

                # Get the index in the flattened advantages/returns list
                data_idx = env_idx * self.n_agents + agent_idx

                if data_idx >= len(all_advantages):
                    continue

                # Normalize advantages for this agent in this environment
                advantages = all_advantages[data_idx]
                advantages = (advantages - advantages.mean()) / (
                    advantages.std() + 1e-8
                )

                # Stack data
                obs = torch.stack(self.observations[env_idx][agent_idx])
                actions = torch.stack(self.actions[env_idx][agent_idx])
                old_log_probs = torch.stack(self.log_probs[env_idx][agent_idx])
                returns = all_returns[data_idx]
                global_states = torch.stack(self.global_states[env_idx])

                # Append to agent-specific lists
                agent_obs.append(obs)
                agent_global_states.append(global_states)
                agent_actions.append(actions)
                agent_old_log_probs.append(old_log_probs)
                agent_returns.append(returns)
                agent_advantages.append(advantages)

            if len(agent_obs) == 0:
                continue  # No data for this agent

            # Concatenate data for this agent from all environments
            obs_combined = torch.cat(agent_obs, dim=0).detach()
            global_states_combined = torch.cat(agent_global_states, dim=0).detach()
            actions_combined = torch.cat(agent_actions, dim=0).detach()
            old_log_probs_combined = torch.cat(agent_old_log_probs, dim=0).detach()
            returns_combined = torch.cat(agent_returns, dim=0)
            advantages_combined = torch.cat(agent_advantages, dim=0)

            # Create dataset for this agent
            dataset = TensorDataset(
                obs_combined,
                global_states_combined,
                actions_combined,
                old_log_probs_combined,
                returns_combined,
                advantages_combined,
            )
            dataloader = DataLoader(dataset, batch_size=minibatch_size, shuffle=True)

            # Train for multiple epochs
            for epoch in range(epochs):
                for batch in dataloader:
                    (
                        batch_obs,
                        batch_global_states,
                        batch_actions,
                        batch_old_log_probs,
                        batch_returns,
                        batch_advantages,
                    ) = batch

                    # Forward pass
                    log_probs, values, entropy = self.network.evaluate_actions(
                        batch_obs, batch_global_states, batch_actions, agent_idx
                    )

                    # PPO objective
                    ratio = torch.exp(log_probs.squeeze(-1) - batch_old_log_probs)
                    surr1 = ratio * batch_advantages
                    surr2 = (
                        torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon)
                        * batch_advantages
                    )
                    policy_loss = -torch.min(surr1, surr2).mean()

                    # Value loss
                    value_loss = F.mse_loss(values, batch_returns)

                    # Entropy loss
                    entropy_loss = -entropy.mean()

                    # Total loss
                    loss = (
                        policy_loss
                        + self.value_coef * value_loss
                        + self.entropy_coef * entropy_loss
                    )

                    # Optimize
                    self.optimizer.zero_grad()
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(
                        self.network.parameters(), self.grad_clip
                    )
                    self.optimizer.step()

                    # Update statistics
                    stats["total_loss"] += loss.item()
                    stats["policy_loss"] += policy_loss.item()
                    stats["value_loss"] += value_loss.item()
                    stats["entropy_loss"] += entropy_loss.item()
                    num_updates += 1

        return stats, num_updates

    def update(self, next_value=0, minibatch_size=128, epochs=10):
        """Update all agents using shared critic"""

        # Compute returns and advantages
        all_returns, all_advantages = self.compute_returns_and_advantages(next_value)

        # Update each agent (or all at once if sharing actor)
        if self.share_actor:
            stats, num_updates = self.update_shared(
                all_advantages, all_returns, minibatch_size, epochs
            )

        else:
            stats, num_updates = self.update_independent_actors(
                all_advantages, all_returns, minibatch_size, epochs
            )

        # Update old network
        self.network_old.load_state_dict(self.network.state_dict())

        # Reset buffers
        self.reset_buffers()

        # Average statistics
        for key in stats:
            stats[key] /= max(1, num_updates)

        return stats
