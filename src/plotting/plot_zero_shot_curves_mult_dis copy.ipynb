{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set font properties for entire plot\n",
    "fontsize=22\n",
    "plt.rcParams.update({\n",
    "    'font.size': fontsize,\n",
    "    'axes.labelsize': fontsize,\n",
    "    'axes.titlesize': fontsize,\n",
    "    'legend.fontsize': fontsize,\n",
    "    'xtick.labelsize': fontsize,\n",
    "    'ytick.labelsize': fontsize,\n",
    "})\n",
    "\n",
    "from math import gcd\n",
    "\n",
    "plotting_dir = Path().resolve()\n",
    "config_dir = plotting_dir / \"ppo_config.yaml\"\n",
    "\n",
    "with open(config_dir, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Group data by batch first, then by experiment, then by n_agents, then by disabled_units\n",
    "batch_data = {batch: defaultdict(lambda: defaultdict(lambda: defaultdict(list))) for batch in config[\"batches\"]}\n",
    "reward_name = \"dist_rewards\"\n",
    "\n",
    "# First, collect all data points across trials\n",
    "for batch in config[\"batches\"]:\n",
    "    for experiment in config[\"experiments\"]:\n",
    "        for trial in config[\"trials\"]:\n",
    "            evaluation_file_name = f\"disabled_mask_eval\"\n",
    "            checkpoint_path = Path(f\"{config['base_path']}/{batch}/{experiment}/{trial}/logs/{evaluation_file_name}.dat\")\n",
    "\n",
    "            if checkpoint_path.is_file():\n",
    "                with open(checkpoint_path, \"rb\") as handle:\n",
    "                    data = pickle.load(handle)\n",
    "                    \n",
    "                    # data structure: {n_agents: {disabled_units: {'dist_rewards': [...], ...}}}\n",
    "                    for n_agents in data.keys():\n",
    "                        for disabled_units in data[n_agents].keys():\n",
    "                            # Append rewards from this trial to the list\n",
    "                            batch_data[batch][experiment][n_agents][disabled_units].extend(\n",
    "                                data[n_agents][disabled_units][reward_name]\n",
    "                            )\n",
    "                        \n",
    "                print(f\"Added trial {trial} data to {batch}/{experiment}\")\n",
    "\n",
    "# Extract training sizes from batch names\n",
    "batch_training_sizes = {}\n",
    "for batch in config[\"batches\"]:\n",
    "    if \"8\" in batch or batch == config[\"batches\"][0]:\n",
    "        batch_training_sizes[batch] = 8\n",
    "    elif \"16\" in batch or batch == config[\"batches\"][1]:\n",
    "        batch_training_sizes[batch] = 16\n",
    "\n",
    "# Group by scale factor (evaluation_size / training_size)\n",
    "scale_factor_data = defaultdict(list)  # {scale_factor: [(batch, n_agents), ...]}\n",
    "\n",
    "for batch in config[\"batches\"]:\n",
    "    training_size = batch_training_sizes[batch]\n",
    "    for experiment in config[\"experiments\"]:\n",
    "        for n_agents in batch_data[batch][experiment].keys():\n",
    "            scale_factor = n_agents / training_size\n",
    "            scale_factor_data[scale_factor].append((batch, n_agents, experiment))\n",
    "\n",
    "# Sort scale factors\n",
    "sorted_scale_factors = sorted(scale_factor_data.keys())\n",
    "\n",
    "# Collect all subplot information\n",
    "subplot_info = []  # List of (scale_factor, batch, n_agents) tuples\n",
    "\n",
    "for scale_factor in sorted_scale_factors:\n",
    "    batch_agent_pairs = scale_factor_data[scale_factor]\n",
    "    unique_batches = list(set([batch for batch, _, _ in batch_agent_pairs]))\n",
    "    \n",
    "    for batch in unique_batches:\n",
    "        # Get n_agents for this batch at this scale factor\n",
    "        n_agents = None\n",
    "        for b, n, _ in batch_agent_pairs:\n",
    "            if b == batch:\n",
    "                n_agents = n\n",
    "                break\n",
    "        if n_agents is not None:\n",
    "            # Calculate reduced ratio\n",
    "            training_size = batch_training_sizes[batch]\n",
    "            ratio_gcd = gcd(training_size, n_agents)\n",
    "            reduced_training = training_size // ratio_gcd\n",
    "            reduced_n_agents = n_agents // ratio_gcd\n",
    "            \n",
    "            # Only include ratios 1:1 and 2:3\n",
    "            if (reduced_training == 1 and reduced_n_agents == 1) or \\\n",
    "               (reduced_training == 2 and reduced_n_agents == 3):\n",
    "                subplot_info.append((scale_factor, batch, n_agents))\n",
    "\n",
    "# Create one large 2x2 figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(22, 16), sharey=True)\n",
    "\n",
    "# Use a different color for each experiment\n",
    "experiment_colors = {}\n",
    "color_idx = 0\n",
    "\n",
    "# Store line objects and labels for shared legend\n",
    "all_lines = []\n",
    "all_labels = [\"gcn_mixed\", \"gat_mixed\", \"graph_transformer_mixed\", \"gcn_full\", \"gat_full\", \"graph_transformer_full\"]\n",
    "\n",
    "# Calculate normalization parameters for each [batch][n_agents] combination\n",
    "normalization_params = {}\n",
    "\n",
    "for batch in config[\"batches\"]:\n",
    "    normalization_params[batch] = {}\n",
    "    for experiment in config[\"experiments\"]:\n",
    "        for n_agents in batch_data[batch][experiment].keys():\n",
    "            if n_agents not in normalization_params[batch]:\n",
    "                # First time seeing this n_agents for this batch\n",
    "                # Collect all means across all experiments for this [batch][n_agents]\n",
    "                all_means_for_batch_nagents = []\n",
    "                \n",
    "                for exp in config[\"experiments\"]:\n",
    "                    if n_agents in batch_data[batch][exp]:\n",
    "                        disabled_units_list = sorted(batch_data[batch][exp][n_agents].keys())\n",
    "                        for disabled_units in disabled_units_list:\n",
    "                            rewards = batch_data[batch][exp][n_agents][disabled_units]\n",
    "                            if rewards:\n",
    "                                all_means_for_batch_nagents.append(np.mean(rewards))\n",
    "                \n",
    "                # Calculate normalization parameters for this [batch][n_agents]\n",
    "                if all_means_for_batch_nagents and max(all_means_for_batch_nagents) != min(all_means_for_batch_nagents):\n",
    "                    min_mean = min(all_means_for_batch_nagents)\n",
    "                    max_mean = max(all_means_for_batch_nagents)\n",
    "                    range_mean = max_mean - min_mean\n",
    "                else:\n",
    "                    min_mean = 0\n",
    "                    range_mean = 1\n",
    "                \n",
    "                normalization_params[batch][n_agents] = {\n",
    "                    'min': min_mean,\n",
    "                    'range': range_mean\n",
    "                }\n",
    "\n",
    "# Plot each subplot\n",
    "for subplot_idx, (scale_factor, batch, n_agents) in enumerate(subplot_info):\n",
    "    if subplot_idx >= 8:  # Only 8 subplots in 2x4 grid\n",
    "        break\n",
    "    \n",
    "    row = subplot_idx // 2\n",
    "    col = subplot_idx % 2\n",
    "\n",
    "    # Swap columns: trained on 8 goes to left (col 0), trained on 16 goes to right (col 1)\n",
    "    training_size = batch_training_sizes[batch]\n",
    "    if training_size == 16:\n",
    "        col = 1  # Right column for 16-unit training\n",
    "    else:\n",
    "        col = 0  # Left column for 8-unit training\n",
    "\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    # Get training size for this batch\n",
    "    training_size = batch_training_sizes[batch]\n",
    "    \n",
    "    # Calculate reduced ratio\n",
    "    ratio_gcd = gcd(training_size, n_agents)\n",
    "    reduced_training = training_size // ratio_gcd\n",
    "    reduced_n_agents = n_agents // ratio_gcd\n",
    "    \n",
    "    # Set subplot title with reduced ratio\n",
    "    ax.set_title(f\"Trained on {training_size} â†’ Evaluated on {n_agents} Salp-Units ({reduced_training}:{reduced_n_agents})\")\n",
    "    \n",
    "    # Set y-label for leftmost plots\n",
    "    if col == 0:\n",
    "        ax.set_ylabel(\"Normalized Mean Distance Reward\")\n",
    "    \n",
    "    # Get normalization parameters for this [batch][n_agents]\n",
    "    min_mean = normalization_params[batch][n_agents]['min']\n",
    "    range_mean = normalization_params[batch][n_agents]['range']\n",
    "    \n",
    "    # Collect experiment data\n",
    "    experiment_data = {}\n",
    "    \n",
    "    for experiment in config[\"experiments\"]:\n",
    "        # Skip if no data for this experiment and n_agents\n",
    "        if n_agents not in batch_data[batch][experiment]:\n",
    "            print(f\"No data for {batch}/{experiment}/n_agents={n_agents}\")\n",
    "            continue\n",
    "        \n",
    "        # Get sorted list of disabled unit counts\n",
    "        disabled_units_list = sorted(batch_data[batch][experiment][n_agents].keys())\n",
    "        \n",
    "        if not disabled_units_list:\n",
    "            continue\n",
    "        \n",
    "        # Calculate mean and standard error for each disabled_units count\n",
    "        means = []\n",
    "        errors = []\n",
    "        for disabled_units in disabled_units_list:\n",
    "            rewards = batch_data[batch][experiment][n_agents][disabled_units]\n",
    "            if rewards:\n",
    "                means.append(np.mean(rewards))\n",
    "                errors.append(np.std(rewards) / np.sqrt(len(rewards)))\n",
    "            else:\n",
    "                means.append(0)\n",
    "                errors.append(0)\n",
    "        \n",
    "        experiment_data[experiment] = {\n",
    "            'disabled_units': disabled_units_list,\n",
    "            'means': means,\n",
    "            'errors': errors\n",
    "        }\n",
    "    \n",
    "    # Plot with [batch][n_agents]-specific normalization\n",
    "    for experiment in config[\"experiments\"]:\n",
    "        if experiment not in experiment_data:\n",
    "            continue\n",
    "            \n",
    "        # Get consistent color for this experiment\n",
    "        if experiment not in experiment_colors:\n",
    "            experiment_colors[experiment] = plt.cm.tab10(color_idx+1 % 10)\n",
    "            color_idx += 1\n",
    "        color = experiment_colors[experiment]\n",
    "        \n",
    "        data = experiment_data[experiment]\n",
    "        disabled_units_list = data['disabled_units']\n",
    "        means = data['means']\n",
    "        errors = data['errors']\n",
    "        \n",
    "        # Normalize using [batch][n_agents]-specific min/max\n",
    "        if range_mean > 0:\n",
    "            normalized_means = [(m - min_mean) / range_mean for m in means]\n",
    "            normalized_errors = [e / range_mean for e in errors]\n",
    "        else:\n",
    "            normalized_means = means\n",
    "            normalized_errors = errors\n",
    "        \n",
    "        # Print sample counts\n",
    "        sample_counts = [len(batch_data[batch][experiment][n_agents][d]) for d in disabled_units_list]\n",
    "        print(f\"{batch}/{experiment}/n_agents={n_agents}: {sample_counts} samples per disabled count\")\n",
    "        \n",
    "        # Plot with error bars\n",
    "        line = ax.errorbar(\n",
    "            disabled_units_list[:8],\n",
    "            normalized_means[:8],\n",
    "            yerr=normalized_errors[:8],\n",
    "            fmt=\"o-\",\n",
    "            linewidth=2,\n",
    "            elinewidth=1,\n",
    "            markersize=6,\n",
    "            capsize=5,\n",
    "            color=color,\n",
    "            ecolor=color,\n",
    "            label=experiment\n",
    "        )\n",
    "        \n",
    "        # Store line and label for shared legend (only from first subplot)\n",
    "        if subplot_idx == 0:\n",
    "            all_lines.append(line[0])\n",
    "    \n",
    "    # Only add x-label to bottom row subplots\n",
    "    if row == 1:  # Bottom row (row 3 out of 0-3)\n",
    "        ax.set_xlabel(\"Number of Disabled Salp Units\")\n",
    "    \n",
    "    if disabled_units_list:\n",
    "        ax.set_xticks(disabled_units_list[:8])\n",
    "    \n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add horizontal line at 0.90\n",
    "    line_90 = ax.axhline(y=0.90, color='black', linestyle='--', linewidth=2)\n",
    "    \n",
    "    # Add legend only for the horizontal line in each subplot\n",
    "    ax.legend([line_90], ['90% performance'], loc='upper right', fontsize=18)\n",
    "\n",
    "# Create a single legend for the entire figure\n",
    "leg = fig.legend(all_lines, all_labels, \n",
    "           loc='lower center',  \n",
    "           bbox_to_anchor=(0.5, -0.06),  \n",
    "           ncol=len(all_labels),\n",
    "           markerscale=2)\n",
    "\n",
    "# Make legend lines thicker\n",
    "for line in leg.get_lines():\n",
    "    line.set_linewidth(6)  # Adjust thickness here\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle(f\"Performance vs Disabled Units Across Different Ratios (train-length:evaluation-length)\", \n",
    "             fontsize=24, y=0.94)\n",
    "\n",
    "# Adjust layout to make room for the legend and title\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.06, top=0.87)\n",
    "\n",
    "plt.savefig(f\"performance_vs_disabled_units_all_scales.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scalable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
