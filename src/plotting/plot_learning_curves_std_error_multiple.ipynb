{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "import yaml\n",
    "\n",
    "fontsize = 22\n",
    "plt.rcParams.update({\n",
    "    'font.size': fontsize,\n",
    "    'axes.labelsize': fontsize,\n",
    "    'axes.titlesize': fontsize,\n",
    "    'legend.fontsize': fontsize,\n",
    "    'xtick.labelsize': fontsize,\n",
    "    'ytick.labelsize': fontsize,\n",
    "})\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "plotting_dir = Path().resolve()\n",
    "config_dir = plotting_dir / \"ppo_config.yaml\"\n",
    "\n",
    "with open(config_dir, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Create a figure with two subplots stacked vertically\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 14), sharex=True)  # sharex=True makes x-axis scale the same\n",
    "\n",
    "# Store data by batch and experiment\n",
    "batch_data = {}\n",
    "for batch in config[\"batches\"]:\n",
    "    batch_data[batch] = defaultdict(list)\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "# First pass: collect data for each experiment grouped by batch\n",
    "for i, batch in enumerate(config[\"batches\"]):\n",
    "    datapoints = config[\"datapoints\"][i]\n",
    "    for experiment in config[\"experiments\"]:\n",
    "        for trial in config[\"trials\"]:\n",
    "            checkpoint_path = Path(f\"{config['base_path']}/{batch}/{experiment}/{trial}/logs/train.dat\")\n",
    "\n",
    "            if checkpoint_path.is_file():\n",
    "                with open(checkpoint_path, \"rb\") as handle:\n",
    "                    data = pickle.load(handle)\n",
    "                    # data = data[\"rewards_per_iteration\"][:datapoints]\n",
    "\n",
    "                    data = data[\"rewards_per_episode\"][:config[\"datapoints\"][i]]\n",
    "                    \n",
    "                    # Apply moving average\n",
    "                    if len(data) > config[\"moving_avg_window_size\"]:\n",
    "                        smoothed_data = moving_average(data, config[\"moving_avg_window_size\"])\n",
    "                        batch_data[batch][experiment].append(smoothed_data)\n",
    "                        max_len = max(max_len, len(smoothed_data))\n",
    "                    else:\n",
    "                        print(f\"Warning: {batch}/{experiment}/{trial} has too few data points for smoothing\")\n",
    "\n",
    "# Second pass: pad arrays to the same length if needed\n",
    "for batch in batch_data:\n",
    "    for exp in batch_data[batch]:\n",
    "        padded_data = []\n",
    "        for trial_data in batch_data[batch][exp]:\n",
    "            if len(trial_data) < max_len:\n",
    "                # Pad with NaN\n",
    "                padding = np.full(max_len - len(trial_data), np.nan)\n",
    "                padded_data.append(np.concatenate([trial_data, padding]))\n",
    "            else:\n",
    "                padded_data.append(trial_data)\n",
    "        \n",
    "        batch_data[batch][exp] = padded_data\n",
    "\n",
    "# Store line objects and labels for combined legend\n",
    "all_lines = []\n",
    "all_labels = [\"mlp\", \"gcn_mixed\", \"gat_mixed\", \"graph_transformer_mixed\", \"gcn_full\", \"gat_full\", \"graph_transformer_full\"]\n",
    "\n",
    "titles = [\"8 Salp-Unit Policy Learning Curve\", \"16 Salp-Unit Policy Learning Curve\"]\n",
    "\n",
    "# Plot each batch in its respective subplot\n",
    "for batch_idx, batch in enumerate(config[\"batches\"]):\n",
    "    ax = axes[batch_idx]\n",
    "    color_idx = 0\n",
    "\n",
    "    \n",
    "    # Add batch name as subplot title\n",
    "    ax.set_title(f\"{titles[batch_idx]}\")\n",
    "    \n",
    "    # First pass: collect all data for this batch to find batch-specific min/max\n",
    "    batch_all_data = []\n",
    "    for exp in batch_data[batch]:\n",
    "        if batch_data[batch][exp]:\n",
    "            data_array = np.array(batch_data[batch][exp])\n",
    "            batch_all_data.append(data_array)\n",
    "    \n",
    "    # Calculate batch-specific normalization parameters\n",
    "    if batch_all_data:\n",
    "        batch_combined = np.concatenate([arr.flatten() for arr in batch_all_data])\n",
    "        batch_min = np.nanmin(batch_combined)\n",
    "        batch_max = np.nanmax(batch_combined)\n",
    "        batch_range = batch_max - batch_min\n",
    "    else:\n",
    "        batch_min = 0\n",
    "        batch_range = 1\n",
    "    \n",
    "    # Second pass: plot with batch-specific normalization\n",
    "    for exp in batch_data[batch]:\n",
    "        if not batch_data[batch][exp]:\n",
    "            print(f\"No data for {batch}/{exp}\")\n",
    "            continue\n",
    "            \n",
    "        # Convert list of arrays to 2D numpy array\n",
    "        data_array = np.array(batch_data[batch][exp])\n",
    "\n",
    "        # Normalize using batch-specific min/max\n",
    "        if batch_range > 0:\n",
    "            data_array = (data_array - batch_min) / batch_range\n",
    "        \n",
    "        # Calculate mean and standard error across trials\n",
    "        mean_rewards = np.nanmean(data_array, axis=0)\n",
    "        \n",
    "        # Standard Error = StdDev / sqrt(n)\n",
    "        n_trials = np.sum(~np.isnan(data_array), axis=0)  # Count non-NaN values at each step\n",
    "        std_rewards = np.nanstd(data_array, axis=0)\n",
    "        se_rewards = std_rewards / np.sqrt(np.maximum(n_trials, 1))  # Avoid division by zero\n",
    "        \n",
    "        # X axis\n",
    "        x = np.arange(len(mean_rewards))\n",
    "        \n",
    "        # Get color for this experiment\n",
    "        color = plt.cm.tab10(color_idx % 10)\n",
    "        color_idx += 1\n",
    "        \n",
    "        # Plot mean line\n",
    "        line = ax.plot(x, mean_rewards, linewidth=2, label=exp, color=color)\n",
    "\n",
    "        # Store line and label for combined legend\n",
    "        # Only add to legend from first subplot to avoid duplicates\n",
    "        if batch_idx == 0:\n",
    "            all_lines.append(line[0])\n",
    "        \n",
    "        # Plot standard error band\n",
    "        ax.fill_between(\n",
    "            x,\n",
    "            mean_rewards - se_rewards,\n",
    "            mean_rewards + se_rewards,\n",
    "            alpha=0.3,\n",
    "            color=color\n",
    "        )\n",
    "        \n",
    "        non_nan_count =  0\n",
    "        for datapoint in batch_data[batch][exp]:\n",
    "            non_nan_count += np.sum(~np.isnan(datapoint))\n",
    "\n",
    "        print(f\"{batch}/{exp}: {len(batch_data[batch][exp])} trials, {non_nan_count} datapoints\")\n",
    "\n",
    "    # Add legend, labels, grid to each subplot\n",
    "    # Only add x-label to bottom subplot\n",
    "    if batch_idx == 1:\n",
    "        ax.set_xlabel(\"Episodes\")\n",
    "    \n",
    "    ax.set_ylabel(\"Normalized SCLD Reward\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Create a single legend for the entire figure with 2 rows of ~3-4 items each\n",
    "leg = fig.legend(all_lines, all_labels, \n",
    "           loc='lower center',  \n",
    "           bbox_to_anchor=(0.5, -0.06),  \n",
    "           ncol=3,  # 3 columns = 3 items per row (7 total = 3, 3, 1)\n",
    "           markerscale=2)\n",
    "\n",
    "# Make legend lines thicker\n",
    "for line in leg.get_lines():\n",
    "    line.set_linewidth(6)  # Adjust thickness here\n",
    "\n",
    "# Add more padding at the bottom for the legend\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.12)  # Adjusted for 3-row legend\n",
    "\n",
    "plt.savefig(\"learning_curves_normalized.png\", dpi=800, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
